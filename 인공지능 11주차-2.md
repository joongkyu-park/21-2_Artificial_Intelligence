# 인공지능 11주차-2

23)
Gradient Descent부터 계속

에러를 하나하나로 보는게 아니라 합쳐서(sum) 보는 이유
-> 하나하나에 대해서 잘작동하는게 아니라 모든 input에 대해서 잘 작동하기를 원하기 때문

이 에러를 minimize을 해야한다
-> weight를 optimize를 해야한다
-> 뭐에 대한 최적? respect to what? 대상이 있어야한다.
-> 에러함수에 대한 최적!

19)
한 스텝 한 스텝 갈수록 minimum값에 가고싶은 것

그러나 에러를 minimize하는것이지, error를 0으로 하는 것은 아니다!

21)
에러가 최소가 되는 지점 찾기 위해 내려가는 과정, weight값을 조정하는 과정 -> 학습!
data를 가지고 가지고 학습.
back propagation을 통해 학습.

25, 26)
구체적으로 얘기해보자

input에서 나온 ouput과 타겟의 차이인 error를 통해 back propagation하여서,
weight를 update해서 error를 줄이고 싶은 거다.

그러면 weight를 어떻게 조정하면되나.

여기선 에러를 (타겟-아웃풋)^2로 하였다

가중치에 대한 에러의 미분값으로 계산

27)
계산을 하면 다음과같은 공식을 얻을 수 있다

28)
따라서 새로운 가
![인공지능 11주차-2](images/인공지능%2011주차-2.png)

중치는,
이전의 가중치에 learning weight(알파값) 곱하기 웨이트가 변하면 에러가 얼마나 변하냐(미분값)를 빼준다

30)
예시
⭐️~31까지의 계산과정 쭉 따라가보기

시멘틱 AI(온톨로지기반) 같은 경우에는 AI의 판단이 틀렸을 때, 왜 틀린지 설명이 가능함. rule가지고 하니까.(이래서 ~~ 이 rule을 적용했다~)
그러나 neural net은 설명이 가능하지 않다
+)
요즘 화두 XAI -> 설명가능한 AI
AI가 고양이를 보고 강아지라고 했다.
왜 그렇게 판단했나?에 대한 질문에 답을 할 수 있는 AI에 대한 연구가 화두.(왜냐하면 귀의 좌표는,, 눈의 좌표는 어쩌고….)

다음챕터로

CNN(Convolutinal Neural Network)
neural net 중에 가장 많이 쓰이는 모델. 특히 computer vision 쪽에

3)
고양이가 시각적인 자극에 대해 어떻게 뇌가 반응하는지 연구하는 것이
CNN의 시초가 됨

5)
컴퓨터는 X와 O를 어떻게 구별하나?
사람에 따라 X와 O의 모양을 그리는게 다 다르기 때문에 쉽지 않다

Features match pieces of the image)
픽셀 하나하나에 초점을 맞춰서 비교하는게 아니라,
그룹으로 묶어서, Feature 단위로 묶어서 생각하게 만들어야한다.

여기서는 3가지 feature로 그룹해서, 3가지 feature와 여러군데서 매칭이 되면 X로 판단한다는 뜻

